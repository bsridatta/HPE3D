{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599466618867",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/b-sridatta/hpe3d\" target=\"_blank\">https://app.wandb.ai/b-sridatta/hpe3d</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/b-sridatta/hpe3d/runs/2dk480uj\" target=\"_blank\">https://app.wandb.ai/b-sridatta/hpe3d/runs/2dk480uj</a><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\nwandb: Wandb version 0.9.6 is available!  To upgrade, please run:\nwandb:  $ pip install wandb --upgrade\n[INFO]: Training data loader called\n[INFO]: processing subjects: [9, 11]\nsamples - 109867\n[INFO]: Validation data loader called\n[INFO]: processing subjects: [9, 11]\nsamples - 109867\n"
    }
   ],
   "source": [
    "import atexit\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from src import train_utils\n",
    "from src import viz\n",
    "from src.dataloader import train_dataloader, val_dataloader\n",
    "from src.models import PJPE, weight_init, Critic\n",
    "from src.trainer import training_epoch, validation_epoch\n",
    "from src.callbacks import CallbackList, ModelCheckpoint, Logging, BetaScheduler, Analyze, MaxNorm\n",
    "from src.train import training_specific_args\n",
    "import sys; sys.argv=['']; del sys\n",
    "# Experiment Configuration, Config, is distributed to all the other modules\n",
    "parser = training_specific_args()\n",
    "config = parser.parse_args()\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)\n",
    "\n",
    "# GPU setup\n",
    "use_cuda = config.cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "config.device = device  # Adding device to config, not already in argparse\n",
    "config.num_workers = 4 if use_cuda else 4  # for dataloader\n",
    "\n",
    "# ignore when debugging on cpu\n",
    "if not use_cuda:\n",
    "    # os.environ['WANDB_MODE'] = 'dryrun'  # Doesnt auto sync to project\n",
    "    os.environ['WANDB_TAGS'] = 'CPU'\n",
    "    wandb.init(anonymous='allow', project=\"hpe3d\", config=config)  # to_delete\n",
    "else:\n",
    "    # os.environ['WANDB_MODE'] = 'dryrun'\n",
    "    wandb.init(anonymous='allow', project=\"hpe3d\", config=config)\n",
    "\n",
    "config.logger = wandb\n",
    "config.logger.run.save()\n",
    "config.run_name = config.logger.run.name  # handle name change in wandb\n",
    "# Data loading\n",
    "config.train_subjects = [9, 11]\n",
    "train_loader = train_dataloader(config)\n",
    "config.val_subjects = [9, 11]\n",
    "val_loader = val_dataloader(config)\n",
    "\n",
    "variant = [['2d', '3d']]\n",
    "\n",
    "models = train_utils.get_models(variant, config)  # model instances\n",
    "if config.self_supervised:\n",
    "    critic = Critic()\n",
    "    models['Critic'] = critic\n",
    "optimizers = train_utils.get_optims(variant, models, config)  # optimer for each pair\n",
    "schedulers = train_utils.get_schedulers(optimizers)\n",
    "\n",
    "# For multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'[INFO]: Using {torch.cuda.device_count()} GPUs')\n",
    "    for key in models.keys():\n",
    "        models[key] = torch.nn.DataParallel(models[key])\n",
    "\n",
    "# To CPU or GPU or TODO TPU\n",
    "for key in models.keys():\n",
    "    models[key] = models[key].to(device)\n",
    "    # models[key].apply(weight_init)\n",
    "\n",
    "config.mpjpe_min=float('inf')\n",
    "config.mpjpe_at_min_val=float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] Loaded Checkpoint colorful-planet-2357: Encoder2D @ epoch 1\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder2D:\n\tMissing key(s) in state_dict: \"LBAD_3.w1.weight\", \"LBAD_3.bn1.weight\", \"LBAD_3.bn1.bias\", \"LBAD_3.bn1.running_mean\", \"LBAD_3.bn1.running_var\", \"LBAD_4.w1.weight\", \"LBAD_4.bn1.weight\", \"LBAD_4.bn1.bias\", \"LBAD_4.bn1.running_mean\", \"LBAD_4.bn1.running_var\". \n\tsize mismatch for enc_inp_block.0.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for enc_inp_block.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.w1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for LBAD_1.bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.w1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for LBAD_2.bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc_mean.weight: copying a param with shape torch.Size([51, 1024]) from checkpoint, the shape in current model is torch.Size([100, 512]).\n\tsize mismatch for fc_mean.bias: copying a param with shape torch.Size([51]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for fc_logvar.weight: copying a param with shape torch.Size([51, 1024]) from checkpoint, the shape in current model is torch.Size([100, 512]).\n\tsize mismatch for fc_logvar.bias: copying a param with shape torch.Size([51]) from checkpoint, the shape in current model is torch.Size([100]).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d7d0a3f048de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m cb.setup(config = config, models = models, optimizers = optimizers,\n\u001b[0;32m----> 9\u001b[0;31m             train_loader = train_loader, val_loader = val_loader, variant = variant)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/HPE3D/src/callbacks/base.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"\"\"Called before the training procedure\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/HPE3D/src/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config, models, optimizers, variant, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 print(\n\u001b[1;32m     22\u001b[0m                     f'[INFO] Loaded Checkpoint {config.resume_run}: {model.name} @ epoch {state[\"epoch\"]}')\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Encoder2D:\n\tMissing key(s) in state_dict: \"LBAD_3.w1.weight\", \"LBAD_3.bn1.weight\", \"LBAD_3.bn1.bias\", \"LBAD_3.bn1.running_mean\", \"LBAD_3.bn1.running_var\", \"LBAD_4.w1.weight\", \"LBAD_4.bn1.weight\", \"LBAD_4.bn1.bias\", \"LBAD_4.bn1.running_mean\", \"LBAD_4.bn1.running_var\". \n\tsize mismatch for enc_inp_block.0.weight: copying a param with shape torch.Size([1024, 32]) from checkpoint, the shape in current model is torch.Size([512, 32]).\n\tsize mismatch for enc_inp_block.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for enc_inp_block.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.w1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for LBAD_1.bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_1.bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.w1.weight: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n\tsize mismatch for LBAD_2.bn1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for LBAD_2.bn1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for fc_mean.weight: copying a param with shape torch.Size([51, 1024]) from checkpoint, the shape in current model is torch.Size([100, 512]).\n\tsize mismatch for fc_mean.bias: copying a param with shape torch.Size([51]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for fc_logvar.weight: copying a param with shape torch.Size([51, 1024]) from checkpoint, the shape in current model is torch.Size([100, 512]).\n\tsize mismatch for fc_logvar.bias: copying a param with shape torch.Size([51]) from checkpoint, the shape in current model is torch.Size([100])."
     ]
    }
   ],
   "source": [
    "config.resume_run = \"colorful-planet-2357\"\n",
    "# initiate all required callbacks, keep the order in mind!!!\n",
    "cb = CallbackList([ModelCheckpoint(),\n",
    "                    Logging(),                       \n",
    "                    BetaScheduler(config, strategy=\"cycling\"),\n",
    "                    Analyze(500)])\n",
    "\n",
    "cb.setup(config = config, models = models, optimizers = optimizers,\n",
    "            train_loader = train_loader, val_loader = val_loader, variant = variant)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for n_pair, pair in enumerate(variant):\n",
    "        vae_type=\"_2_\".join(pair)\n",
    "        # model -- encoder, decoder / critic\n",
    "        model=[models[f\"Encoder{pair[0].upper()}\"],\n",
    "                    models[f\"Decoder{pair[1].upper()}\"]]\n",
    "        optimizer = [optimizers[n_pair]]\n",
    "        scheduler = [schedulers[n_pair]]\n",
    "\n",
    "        if config.self_supervised:\n",
    "            model.append(models['Critic'])\n",
    "            optimizer.append(optimizers[-1])\n",
    "            scheduler.append(schedulers[-1])\n",
    "\n",
    "        val_loss = validation_epoch(\n",
    "                config, cb, model, val_loader, epoch, vae_type)\n",
    "\n",
    "\n",
    "        cb.on_epoch_end(config=config, val_loss=val_loss, model=model,\n",
    "                        n_pair=n_pair, optimizers=optimizers, epoch=epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}